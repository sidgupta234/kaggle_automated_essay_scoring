{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e54a4512",
   "metadata": {},
   "source": [
    "# Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "id": "3b936d7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np, pandas as pd, sklearn, matplotlib.pyplot as plt, seaborn as sns\n",
    "import re\n",
    "import math\n",
    "import statistics\n",
    "from scipy.stats import pearsonr\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "label_dict = {1.0:'A', 1.5: 'B', 2: 'C', 2.5: 'D', 3: 'E', 3.5: 'F', 4: 'G', 4.5: 'H', 5: 'I'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "id": "df3ff0a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv('data/train.csv')\n",
    "english_vocab_frequency =  pd.read_csv('vocabulary_data/unigram_freq.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "id": "ac9480ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text_id</th>\n",
       "      <th>full_text</th>\n",
       "      <th>cohesion</th>\n",
       "      <th>syntax</th>\n",
       "      <th>vocabulary</th>\n",
       "      <th>phraseology</th>\n",
       "      <th>grammar</th>\n",
       "      <th>conventions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0016926B079C</td>\n",
       "      <td>I think that students would benefit from learn...</td>\n",
       "      <td>3.5</td>\n",
       "      <td>3.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0022683E9EA5</td>\n",
       "      <td>When a problem is a change you have to let it ...</td>\n",
       "      <td>2.5</td>\n",
       "      <td>2.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        text_id                                          full_text  cohesion  \\\n",
       "0  0016926B079C  I think that students would benefit from learn...       3.5   \n",
       "1  0022683E9EA5  When a problem is a change you have to let it ...       2.5   \n",
       "\n",
       "   syntax  vocabulary  phraseology  grammar  conventions  \n",
       "0     3.5         3.0          3.0      4.0          3.0  \n",
       "1     2.5         3.0          2.0      2.0          2.5  "
      ]
     },
     "execution_count": 310,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "id": "a55fd816",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>25%</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>cohesion</th>\n",
       "      <td>3911.0</td>\n",
       "      <td>3.127077</td>\n",
       "      <td>0.662542</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>syntax</th>\n",
       "      <td>3911.0</td>\n",
       "      <td>3.028254</td>\n",
       "      <td>0.644399</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>vocabulary</th>\n",
       "      <td>3911.0</td>\n",
       "      <td>3.235745</td>\n",
       "      <td>0.583148</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>phraseology</th>\n",
       "      <td>3911.0</td>\n",
       "      <td>3.116850</td>\n",
       "      <td>0.655997</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>grammar</th>\n",
       "      <td>3911.0</td>\n",
       "      <td>3.032856</td>\n",
       "      <td>0.699841</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>conventions</th>\n",
       "      <td>3911.0</td>\n",
       "      <td>3.081053</td>\n",
       "      <td>0.671450</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              count      mean       std  min  25%  50%  75%  max\n",
       "cohesion     3911.0  3.127077  0.662542  1.0  2.5  3.0  3.5  5.0\n",
       "syntax       3911.0  3.028254  0.644399  1.0  2.5  3.0  3.5  5.0\n",
       "vocabulary   3911.0  3.235745  0.583148  1.0  3.0  3.0  3.5  5.0\n",
       "phraseology  3911.0  3.116850  0.655997  1.0  2.5  3.0  3.5  5.0\n",
       "grammar      3911.0  3.032856  0.699841  1.0  2.5  3.0  3.5  5.0\n",
       "conventions  3911.0  3.081053  0.671450  1.0  2.5  3.0  3.5  5.0"
      ]
     },
     "execution_count": 311,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.describe().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "id": "419f1a86",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I think that students would benefit from learning at home,because they wont have to change and get up early in the morning to shower and do there hair. taking only classes helps them because at there house they'll be pay more attention. they will be comfortable at home.\\n\\nThe hardest part of school is getting ready. you wake up go brush your teeth and go to your closet and look at your cloths. after you think you picked a outfit u go look in the mirror and youll either not like it or you look and see a stain. Then you'll have to change. with the online classes you can wear anything and stay home and you wont need to stress about what to wear.\\n\\nmost students usually take showers before school. they either take it before they sleep or when they wake up. some students do both to smell good. that causes them do miss the bus and effects on there lesson time cause they come late to school. when u have online classes u wont need to miss lessons cause you can get everything set up and go take a shower and when u get out your ready to go.\\n\\nwhen your home your comfortable and you pay attention. it gives then an advantage to be smarter and even pass there classmates on class work. public schools are difficult even if you try. some teacher dont know how to teach it in then way that students understand it. that causes students to fail and they may repeat the class.              \""
      ]
     },
     "execution_count": 312,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.full_text[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "id": "d001242c",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'dict' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [317]\u001b[0m, in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m dataset \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata/train.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      4\u001b[0m english_vocab_frequency \u001b[38;5;241m=\u001b[39m  pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvocabulary_data/unigram_freq.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m----> 5\u001b[0m english_vocab_frequency_dict \u001b[38;5;241m=\u001b[39m  \u001b[38;5;28;43mdict\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mzip\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43menglish_vocab_frequency\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mword\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menglish_vocab_frequency\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcount\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m#print(len(english_vocab_frequency_dict))\u001b[39;00m\n\u001b[1;32m      7\u001b[0m oov_list \u001b[38;5;241m=\u001b[39m []\n",
      "\u001b[0;31mTypeError\u001b[0m: 'dict' object is not callable"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "dataset = pd.read_csv('data/train.csv')\n",
    "english_vocab_frequency =  pd.read_csv('vocabulary_data/unigram_freq.csv')\n",
    "english_vocab_frequency_dict =  dict(zip(english_vocab_frequency['word'], english_vocab_frequency['count']))\n",
    "#print(len(english_vocab_frequency_dict))\n",
    "oov_list = []\n",
    "total_unique_words = []\n",
    "#sentence_number = 0\n",
    "\n",
    "print(dataset['full_text'][0])\n",
    "\n",
    "def clean_dataset(dataset_row):\n",
    "    #dataset_row[\"full_text\"] = dataset_row[\"full_text\"].lower()\n",
    "    dataset_row[\"full_text\"] = dataset_row[\"full_text\"].lower()\n",
    "    dataset_row[\"full_text\"] = re.sub('[\\t\\n\\r]', ' ', dataset_row[\"full_text\"]) #Removing \\n, \\t, \\r from from full_test\n",
    "    dataset_row[\"full_text\"] = re.sub('[^0-9a-z]', ' ', dataset_row[\"full_text\"]) #Replace all symbols except a-z and 0-9 with spaces\n",
    "    dataset_row[\"full_text\"] = re.sub('\\s{2,}', ' ', dataset_row[\"full_text\"]) #Replacing two or more spaces with single space\n",
    "    dataset_row['full_text'] = dataset_row[\"full_text\"].strip() #Removing start or end of line spaces from full_test\n",
    "    return dataset_row['full_text']\n",
    "\n",
    "def avg_unique_words(dataset_row):\n",
    "    dict_ = {}\n",
    "    words = dataset_row['full_text'].split()\n",
    "    for word in words:\n",
    "        dict_[word] = 1\n",
    "        \n",
    "    return len(dict_)/len(words)\n",
    "\n",
    "def word_freq_score(dataset_row, english_vocab_frequency_dict):\n",
    "    global oov_list\n",
    "    global total_unique_words\n",
    "    freq_score = 0\n",
    "    #print(dataset_row)\n",
    "    words = dataset_row['full_text'].split()\n",
    "    global sentence_number\n",
    "    \n",
    "#     if(sentence_number%50 == 0):\n",
    "#         print(\"sentencer_number is \", sentence_number)\n",
    "#     sentence_number+=1\n",
    "    for word in words:\n",
    "#         print(word)\n",
    "        if word in english_vocab_frequency_dict:\n",
    "            #print(english_vocab_frequency[english_vocab_frequency['word'] == word])\n",
    "            total_unique_words.append(word)\n",
    "            freq_score += 1/math.log(english_vocab_frequency_dict[word])\n",
    "        else:\n",
    "            #print(word)\n",
    "            oov_list.append(word)\n",
    "            #freq_score += 1/math.log(english_vocab_frequency_dict['the'])\n",
    "            #freq_score += math.log(1/english_vocab_frequency_dict[word])\n",
    "            #print(word)\n",
    "            \n",
    "    return len(words)/freq_score\n",
    "\n",
    "def vocab_label(dataset_row):\n",
    "    return label_dict[dataset_row['vocabulary']]\n",
    "            \n",
    "dataset['full_text'] = dataset.apply(clean_dataset, axis = 1)\n",
    "dataset['text_length'] = dataset.apply(lambda x: len(x['full_text'].split()), axis = 1)\n",
    "dataset['avg_word_length'] = dataset.apply(lambda x: statistics.mean([len(i) for i in x['full_text'].split()]), axis = 1)\n",
    "dataset['avg_unique_words_per_total_words'] = dataset.apply(avg_unique_words, axis = 1)\n",
    "dataset['word_frequency_score'] = dataset.apply(lambda x: word_freq_score(dataset_row = x, english_vocab_frequency_dict = english_vocab_frequency_dict), axis = 1)\n",
    "dataset['vocab_label'] = dataset.apply(vocab_label, axis = 1)\n",
    "end_time = time.time()\n",
    "print(\"--- %s seconds ---\" % (end_time - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "id": "7402cf18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PearsonRResult(statistic=0.23755775999754644, pvalue=2.6362115691817557e-51)\n"
     ]
    }
   ],
   "source": [
    "#dataset[['vocabulary', 'avg_word_length']].head(10)\n",
    "print(pearsonr(dataset['vocabulary'],dataset['avg_word_length']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "id": "41a8c7d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PearsonRResult(statistic=-0.017353239132762925, pvalue=0.2779342989408944)\n"
     ]
    }
   ],
   "source": [
    "#dataset[['vocabulary', 'avg_word_length']].head(10)\n",
    "print(pearsonr(dataset['vocabulary'],dataset['avg_unique_words_per_total_words']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "id": "73dfcee0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13146"
      ]
     },
     "execution_count": 268,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(oov_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "id": "82984612",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1694355"
      ]
     },
     "execution_count": 269,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(total_unique_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "id": "7697eebf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6842"
      ]
     },
     "execution_count": 270,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(oov_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "id": "ea5cdd81",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14538"
      ]
     },
     "execution_count": 271,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(total_unique_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "id": "05e8aa23",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text_id</th>\n",
       "      <th>full_text</th>\n",
       "      <th>cohesion</th>\n",
       "      <th>syntax</th>\n",
       "      <th>vocabulary</th>\n",
       "      <th>phraseology</th>\n",
       "      <th>grammar</th>\n",
       "      <th>conventions</th>\n",
       "      <th>text_length</th>\n",
       "      <th>avg_word_length</th>\n",
       "      <th>avg_unique_words_per_total_words</th>\n",
       "      <th>word_frequency_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0016926B079C</td>\n",
       "      <td>i think that students would benefit from learn...</td>\n",
       "      <td>3.5</td>\n",
       "      <td>3.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>264</td>\n",
       "      <td>4.125000</td>\n",
       "      <td>0.496212</td>\n",
       "      <td>19.662496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0022683E9EA5</td>\n",
       "      <td>when a problem is a change you have to let it ...</td>\n",
       "      <td>2.5</td>\n",
       "      <td>2.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.5</td>\n",
       "      <td>536</td>\n",
       "      <td>3.875000</td>\n",
       "      <td>0.253731</td>\n",
       "      <td>20.348304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>00299B378633</td>\n",
       "      <td>dear principal if u change the school policy o...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.5</td>\n",
       "      <td>330</td>\n",
       "      <td>3.960606</td>\n",
       "      <td>0.348485</td>\n",
       "      <td>19.986943</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>003885A45F42</td>\n",
       "      <td>the best time in life is when you become yours...</td>\n",
       "      <td>4.5</td>\n",
       "      <td>4.5</td>\n",
       "      <td>4.5</td>\n",
       "      <td>4.5</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>756</td>\n",
       "      <td>4.140212</td>\n",
       "      <td>0.268519</td>\n",
       "      <td>19.867119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0049B1DF5CCC</td>\n",
       "      <td>small act of kindness can impact in other peop...</td>\n",
       "      <td>2.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.5</td>\n",
       "      <td>2.5</td>\n",
       "      <td>234</td>\n",
       "      <td>4.076923</td>\n",
       "      <td>0.448718</td>\n",
       "      <td>20.723091</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        text_id                                          full_text  cohesion  \\\n",
       "0  0016926B079C  i think that students would benefit from learn...       3.5   \n",
       "1  0022683E9EA5  when a problem is a change you have to let it ...       2.5   \n",
       "2  00299B378633  dear principal if u change the school policy o...       3.0   \n",
       "3  003885A45F42  the best time in life is when you become yours...       4.5   \n",
       "4  0049B1DF5CCC  small act of kindness can impact in other peop...       2.5   \n",
       "\n",
       "   syntax  vocabulary  phraseology  grammar  conventions  text_length  \\\n",
       "0     3.5         3.0          3.0      4.0          3.0          264   \n",
       "1     2.5         3.0          2.0      2.0          2.5          536   \n",
       "2     3.5         3.0          3.0      3.0          2.5          330   \n",
       "3     4.5         4.5          4.5      4.0          5.0          756   \n",
       "4     3.0         3.0          3.0      2.5          2.5          234   \n",
       "\n",
       "   avg_word_length  avg_unique_words_per_total_words  word_frequency_score  \n",
       "0         4.125000                          0.496212             19.662496  \n",
       "1         3.875000                          0.253731             20.348304  \n",
       "2         3.960606                          0.348485             19.986943  \n",
       "3         4.140212                          0.268519             19.867119  \n",
       "4         4.076923                          0.448718             20.723091  "
      ]
     },
     "execution_count": 272,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "id": "70e0a526",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PearsonRResult(statistic=-0.3011714512937773, pvalue=8.226281419648244e-83)\n"
     ]
    }
   ],
   "source": [
    "print(pearsonr(dataset['vocabulary'], dataset['word_frequency_score']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "id": "39930f37",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = dataset[['word_frequency_score', 'avg_unique_words_per_total_words', 'avg_word_length']]\n",
    "y = dataset['vocabulary']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.1, random_state=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "id": "a6ffc60b",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Unknown label type: 'continuous'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[0;32mIn [304]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m regressor \u001b[38;5;241m=\u001b[39m LogisticRegression()\n\u001b[0;32m----> 2\u001b[0m \u001b[43mregressor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(regressor\u001b[38;5;241m.\u001b[39mscore(X_test, y_test))\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/feedback_prize_kaggle/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:1146\u001b[0m, in \u001b[0;36mLogisticRegression.fit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m   1136\u001b[0m     _dtype \u001b[38;5;241m=\u001b[39m [np\u001b[38;5;241m.\u001b[39mfloat64, np\u001b[38;5;241m.\u001b[39mfloat32]\n\u001b[1;32m   1138\u001b[0m X, y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_data(\n\u001b[1;32m   1139\u001b[0m     X,\n\u001b[1;32m   1140\u001b[0m     y,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1144\u001b[0m     accept_large_sparse\u001b[38;5;241m=\u001b[39msolver \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mliblinear\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msag\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msaga\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m   1145\u001b[0m )\n\u001b[0;32m-> 1146\u001b[0m \u001b[43mcheck_classification_targets\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1147\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclasses_ \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39munique(y)\n\u001b[1;32m   1149\u001b[0m multi_class \u001b[38;5;241m=\u001b[39m _check_multi_class(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmulti_class, solver, \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclasses_))\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/feedback_prize_kaggle/lib/python3.10/site-packages/sklearn/utils/multiclass.py:200\u001b[0m, in \u001b[0;36mcheck_classification_targets\u001b[0;34m(y)\u001b[0m\n\u001b[1;32m    192\u001b[0m y_type \u001b[38;5;241m=\u001b[39m type_of_target(y, input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    193\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m y_type \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m [\n\u001b[1;32m    194\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbinary\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    195\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmulticlass\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    198\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmultilabel-sequences\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    199\u001b[0m ]:\n\u001b[0;32m--> 200\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnknown label type: \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m y_type)\n",
      "\u001b[0;31mValueError\u001b[0m: Unknown label type: 'continuous'"
     ]
    }
   ],
   "source": [
    "regressor = LogisticRegression()\n",
    "regressor.fit(X_train, y_train)\n",
    "print(regressor.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c317e72",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
